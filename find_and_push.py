import requests
from bs4 import BeautifulSoup
import re
import logging
import time
from urllib.parse import urljoin

# =========================
# é…ç½®åŒºåŸŸ
# =========================
BASE_URL = "https://nodefree.net"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}
TG_BOT_TOKEN = "<ä½ çš„BotToken>"
TG_CHAT_ID = "<ä½ çš„é¢‘é“æˆ–ç”¨æˆ·ID>"

# =========================
# æ—¥å¿—è®¾ç½®
# =========================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

# =========================
# èŠ‚ç‚¹æå–å‡½æ•°
# =========================
def extract_freenodes_links():
    url = "https://freenodes.github.io/freenodes/"
    logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å– Freenodes é¡µé¢: {url}")
    links = set()
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        for a in soup.select('a[href]'):
            href = a['href']
            if any(ext in href for ext in ['.yaml', 'subscribe', 'clash']):
                if not href.startswith('http'):
                    href = urljoin(url, href)
                links.add(href)
        logger.info(f"   ğŸ”— Freenodes æå–åˆ° {len(links)} ä¸ªè®¢é˜…é“¾æ¥")
    except Exception as e:
        logger.error(f"âŒ Freenodes æå–å¤±è´¥: {str(e)}")
    return list(links)

def extract_freefq_links():
    url = "https://freefq.com/free-ssr/"
    logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å– FreeFQ é¡µé¢: {url}")
    links = set()
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        for a in soup.select('a[href]'):
            href = a['href']
            if any(x in href for x in ['subscribe', 'clash', 'yaml']):
                if not href.startswith('http'):
                    href = urljoin(url, href)
                links.add(href)
        logger.info(f"   ğŸ”— FreeFQ æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
    except Exception as e:
        logger.error(f"âŒ FreeFQ æå–å¤±è´¥: {str(e)}")
    return list(links)

def extract_proxypoolss_links():
    url = "https://proxypoolss.pages.dev"
    logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å– ProxyPoolss é¡µé¢: {url}")
    links = set()
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        links.update(re.findall(r'https?://[\w./%-]+\.yaml', resp.text))
        logger.info(f"   ğŸ”— ProxyPoolss æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
    except Exception as e:
        logger.error(f"âŒ ProxyPoolss æå–å¤±è´¥: {str(e)}")
    return list(links)

def extract_clashfree_links():
    base_url = "https://raw.githubusercontent.com/aiboboxx/clashfree/main/"
    files = ["clash.yaml", "clash.meta.yaml"]
    links = [base_url + f for f in files]
    logger.info(f"ğŸŒ ä» GitHub aiboboxx æ·»åŠ é™æ€è®¢é˜…é“¾æ¥: {len(links)}")
    return links

def get_threads_on_page(page_num):
    page_url = f"{BASE_URL}/page/{page_num}" if page_num > 1 else BASE_URL
    logger.info(f"ğŸ” æ­£åœ¨çˆ¬å–é¡µé¢: {page_url}")
    threads = []
    try:
        resp = requests.get(page_url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        for article in soup.select('article'):
            a_tag = article.select_one('h2.entry-title a')
            if a_tag:
                href = a_tag.get('href')
                threads.append(href)
        logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ç¯‡æ–‡ç« ")
    except Exception as e:
        logger.warning(f"âš ï¸ ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°æ–‡ç« ï¼Œè·³è¿‡: {str(e)}")
    return threads

def extract_nodefree_links():
    logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å– NodeFree ç½‘é¡µ")
    links = set()
    for i in range(1, 3):
        threads = get_threads_on_page(i)
        for url in threads:
            try:
                resp = requests.get(url, headers=HEADERS, timeout=10)
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')
                text = soup.get_text()
                found = re.findall(r'https?://[\w./%-]+\.(?:yaml|txt)', text)
                links.update(found)
            except Exception as e:
                logger.warning(f"âš ï¸ æ–‡ç« æŠ“å–å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
    return list(links)

def is_valid_subscription(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        if resp.status_code != 200:
            logger.warning(f"    âŒ HTTP {resp.status_code}: {url}")
            return False
        if not re.search(r'(proxy-groups|proxies|server|name)', resp.text, re.IGNORECASE):
            logger.warning(f"    âŒ æ— æ•ˆè®¢é˜… (æ— VPNé…ç½®): {url}")
            return False
        logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜…: {url}")
        return True
    except Exception as e:
        logger.warning(f"    âŒ éªŒè¯å¤±è´¥: {url} -> {str(e)}")
        return False

def push_to_telegram(message):
    api_url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TG_CHAT_ID,
        "text": message,
        "parse_mode": "HTML"
    }
    try:
        resp = requests.post(api_url, data=payload)
        logger.info(f"âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨é€å¤±è´¥: {str(e)}")

# =========================
# ä¸»æµç¨‹
# =========================
def main():
    logger.info("=" * 50)
    logger.info(f"ğŸŒ NodeFree + Freenodes çˆ¬è™«å¯åŠ¨ - {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 50)

    all_links = set()

    all_links.update(extract_nodefree_links())
    all_links.update(extract_freenodes_links())
    all_links.update(extract_freefq_links())
    all_links.update(extract_proxypoolss_links())
    all_links.update(extract_clashfree_links())

    logger.info(f"\nğŸ” å…±æå–åˆ° {len(all_links)} æ¡è®¢é˜…é“¾æ¥ï¼Œå¼€å§‹éªŒè¯...")

    valid_links = []
    for link in all_links:
        logger.info(f"ğŸ” æ­£åœ¨éªŒè¯é“¾æ¥: {link}")
        if is_valid_subscription(link):
            valid_links.append(link)

    logger.info(f"\nâœ”ï¸ éªŒè¯å®Œæˆï¼å…± {len(valid_links)} æ¡æœ‰æ•ˆè®¢é˜…é“¾æ¥")

    with open("valid_links.txt", "w") as f:
        for link in valid_links:
            f.write(link + "\n")
    logger.info("ğŸ“„ ç»“æœå·²ä¿å­˜åˆ° valid_links.txt")

    if valid_links:
        msg = "\n".join(valid_links)
        push_to_telegram("<b>ğŸ“¡ ä»Šæ—¥æœ‰æ•ˆè®¢é˜…é“¾æ¥:</b>\n" + msg)

    logger.info("\nâœ… ä»»åŠ¡å®Œæˆï¼")

if __name__ == '__main__':
    main()
