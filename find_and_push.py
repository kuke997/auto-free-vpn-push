import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
from urllib.parse import urljoin, quote
import time
import random
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
BASE_URL = "https://nodefree.net"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0",
    "TE": "Trailers"
}

def get_threads_on_page(page):
    if page == 1:
        url = BASE_URL
    else:
        url = f"{BASE_URL}/page/{page}"
    
    logger.info(f"ğŸ” æ­£åœ¨çˆ¬å–é¡µé¢: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        threads = []

        for article in soup.select('div.topic-list-item'):
            title_link = article.select_one('a.title')
            if title_link:
                href = title_link.get('href')
                if href:
                    full_url = urljoin(BASE_URL, href)
                    threads.append(full_url)
        
        logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ç¯‡æ–‡ç« ")
        return threads
    except Exception as e:
        logger.error(f"âš ï¸ è·å–é¡µé¢å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def extract_subscription_links(url):
    logger.info(f"ğŸ“ æ­£åœ¨è§£ææ–‡ç« : {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = set()

        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if any(pattern in href for pattern in ['.yaml', '.yml', '.txt', 'clash', 'v2ray', 'subscribe', 'nodefree']):
                if not href.startswith('http'):
                    href = urljoin(url, href)
                links.add(href)

        content = soup.select_one('div.content') or soup.select_one('div.post-content')
        if content:
            potential_links = re.findall(r'https?://[^\s"\']+', content.text)
            for link in potential_links:
                if any(pattern in link for pattern in ['.yaml', '.yml', '.txt', 'clash', 'v2ray', 'subscribe', 'nodefree']):
                    links.add(link)

        for code_block in soup.select('pre, code'):
            base64_links = re.findall(r'(?:ss|ssr|vmess|trojan)://[a-zA-Z0-9+/]+={0,2}', code_block.text)
            links.update(base64_links)
            text_links = re.findall(r'https?://[^\s"\']+', code_block.text)
            for link in text_links:
                if any(pattern in link for pattern in ['.yaml', '.yml', '.txt', 'clash', 'v2ray', 'subscribe']):
                    links.add(link)

        logger.info(f"   ğŸ”— æå–åˆ° {len(links)} ä¸ªè®¢é˜…é“¾æ¥")
        return list(links)
    except Exception as e:
        logger.error(f"âš ï¸ è§£æå¸–å­å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def validate_subscription(url):
    logger.info(f"ğŸ” æ­£åœ¨éªŒè¯é“¾æ¥: {url}")
    
    try:
        time.sleep(random.uniform(0.5, 1.5))
        if url.startswith('//'):
            url = 'https:' + url
        elif not url.startswith('http'):
            url = 'https://' + url
            
        res = requests.get(url, headers=HEADERS, timeout=15)
        if res.status_code != 200:
            logger.warning(f"    âŒ HTTP {res.status_code}: {url}")
            return False
        
        content = res.text
        vpn_keywords = ["proxies", "proxy-providers", "vmess", "ss://", "trojan", "vless", "clash", "port:"]
        for keyword in vpn_keywords:
            if keyword.lower() in content.lower():
                logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜…: {url}")
                return True

        if re.search(r'^[A-Za-z0-9+/]+={0,2}$', content.strip()):
            logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜… (Base64ç¼–ç ): {url}")
            return True

        logger.warning(f"    âŒ æ— æ•ˆè®¢é˜… (æ— VPNé…ç½®): {url}")
        return False
    except Exception as e:
        logger.error(f"    âŒ éªŒè¯å¼‚å¸¸: {url}ï¼Œé”™è¯¯: {str(e)}")
        return False

def get_freenodes_links():
    url = "https://freenodes.github.io/freenodes/"
    logger.info(f"ğŸŒ æ­£åœ¨çˆ¬å– Freenodes é¡µé¢: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = set()

        for a in soup.find_all("a", href=True):
            href = a['href']
            if any(ext in href for ext in ['.yaml', '.yml', '.txt', 'subscribe', 'clash']):
                full_url = urljoin(url, href)
                links.add(full_url)
            if any(proto in href for proto in ['ss://', 'vmess://', 'trojan://']):
                links.add(href)

        logger.info(f"   ğŸ”— Freenodes æå–åˆ° {len(links)} ä¸ªè®¢é˜…é“¾æ¥")
        return list(links)
    except Exception as e:
        logger.error(f"âš ï¸ Freenodes é¡µé¢è§£æå¤±è´¥: {str(e)}")
        return []

async def send_to_telegram(bot_token, channel_id, urls):
    if not urls:
        logger.warning("âŒ æ— æœ‰æ•ˆé“¾æ¥ï¼Œè·³è¿‡æ¨é€")
        return
    
    text = "ğŸ†• <b>æœ€æ–°å…è´¹VPNè®¢é˜…é“¾æ¥</b>\n\n"
    text += "æ›´æ–°æ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n\n"
    text += "ä»¥ä¸‹æ˜¯å¯ç›´æ¥å¯¼å…¥VPNå®¢æˆ·ç«¯çš„è®¢é˜…é“¾æ¥ï¼š\n\n"
    
    for i, u in enumerate(urls[:10], 1):
        safe = quote(u, safe=":/?=&")
        display_name = u.split('/')[-1] if '/' in u else u
        text += f"{i}. <code>{display_name}</code>\n"
        text += f"   <a href=\"{safe}\">ç‚¹å‡»å¤åˆ¶è®¢é˜…é“¾æ¥</a>\n\n"
    
    text += "âš ï¸ ä»…ä¾›å­¦ä¹ ä½¿ç”¨ï¼Œè¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„\n"
    text += "ğŸ”’ è®¢é˜…é“¾æ¥æœ‰æ•ˆæœŸé€šå¸¸ä¸º1-7å¤©"

    if len(text) > 4096:
        text = text[:4000] + "\n\n...ï¼ˆéƒ¨åˆ†é“¾æ¥å·²çœç•¥ï¼‰"

    bot = Bot(token=bot_token)
    try:
        await bot.send_message(
            chat_id=channel_id,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True,
        )
        logger.info("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨é€å¤±è´¥: {str(e)}")

async def main():
    logger.info("="*50)
    logger.info(f"ğŸŒ NodeFree + Freenodes çˆ¬è™«å¯åŠ¨ - {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*50)
    
    all_links = set()
    valid_links = []

    freenodes_links = get_freenodes_links()
    all_links.update(freenodes_links)
    time.sleep(random.uniform(1, 2))
    
    for page in range(1, 3):
        threads = get_threads_on_page(page)
        if not threads:
            logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« ï¼Œè·³è¿‡")
            continue
        time.sleep(random.uniform(1, 3))
        for t in threads:
            subs = extract_subscription_links(t)
            all_links.update(subs)
            time.sleep(random.uniform(0.5, 2))
    
    logger.info(f"\nğŸ” å…±æå–åˆ° {len(all_links)} æ¡è®¢é˜…é“¾æ¥ï¼Œå¼€å§‹éªŒè¯...")
    
    for link in all_links:
        if validate_subscription(link):
            valid_links.append(link)
    
    logger.info(f"\nâœ”ï¸ éªŒè¯å®Œæˆï¼å…± {len(valid_links)} æ¡æœ‰æ•ˆè®¢é˜…é“¾æ¥")
    
    if valid_links:
        with open("valid_links.txt", "w", encoding="utf-8") as f:
            for v in valid_links:
                f.write(v + "\n")
        logger.info("ğŸ“„ ç»“æœå·²ä¿å­˜åˆ° valid_links.txt")
    else:
        logger.warning("ğŸ“„ æ— æœ‰æ•ˆé“¾æ¥ï¼Œä¸ä¿å­˜æ–‡ä»¶")

    if BOT_TOKEN and CHANNEL_ID and valid_links:
        logger.info("\nğŸ“¤ æ­£åœ¨æ¨é€ç»“æœåˆ°Telegram...")
        await send_to_telegram(BOT_TOKEN, CHANNEL_ID, valid_links)
    elif valid_links:
        logger.warning("\nâŒ æœªè®¾ç½®BOT_TOKENæˆ–CHANNEL_IDï¼Œè·³è¿‡æ¨é€")

    logger.info("\nâœ… ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
