import os
import re
import time
import asyncio
import logging
from urllib.parse import quote, urljoin

import requests
from telegram import Bot
from telegram.constants import ParseMode
from requests_html import AsyncHTMLSession

# ---------------------- æ—¥å¿—é…ç½® ----------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ---------------------- ç¯å¢ƒå˜é‡ & å¸¸é‡ ----------------------
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

FREEFQ_CATEGORY_URL = "https://freefq.com/free-ssr/"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"


# ---------------------- å¸®åŠ©å‡½æ•°ï¼šåˆ¤æ–­â€œæ˜¯å¦çœŸçš„æ˜¯è®¢é˜…é“¾æ¥â€ ----------------------
def is_subscription_link(link: str) -> bool:
    """
    åªä¿ç•™ä»¥ä¸‹ä¸¤ç±»é“¾æ¥ï¼š
      1. åè®®ç±»ï¼šä»¥ ssr://ã€ss://ã€vmess://ã€trojan:// å¼€å¤´
      2. æ–‡ä»¶åç¼€ç±»ï¼šä»¥ .yamlã€.ymlã€.txtã€.json ç»“å°¾ï¼ˆå¯å¸¦ ? æŸ¥è¯¢å‚æ•°ï¼‰
    """
    link = link.strip()
    # åè®®ç±»
    if re.match(r'^(ssr|ss|vmess|trojan)://', link):
        return True
    # æ–‡ä»¶åç¼€ç±»ï¼ˆåé¢å¯ä»¥æœ‰ ?queryï¼‰
    if re.search(r'\.(yaml|yml|txt|json)(?:\?[^ ]*)?$', link):
        return True
    return False


# ---------------------- æå–åˆ†ç±»é¡µæ–‡ç« é“¾æ¥ ----------------------
async def extract_post_urls(asession):
    """
    æ¸²æŸ“åˆ†ç±»é¡µåï¼Œä» r.html.html é‡Œç²¾å‡†åŒ¹é… â€œ/free-ssr/YYYY/MM/DD/xxx.htmlâ€ æ ¼å¼çš„æ–‡ç« é“¾æ¥ï¼Œ
    æ’é™¤æ‰ index_*.html è¿™äº›åˆ†é¡µ/ç´¢å¼•é¡µã€‚
    """
    try:
        r = await asession.get(FREEFQ_CATEGORY_URL, headers={"User-Agent": USER_AGENT})
        # ç»™ JS è¶³å¤Ÿæ—¶é—´åŠ è½½
        await r.html.arender(timeout=30, sleep=2)

        html_source = r.html.html or ""
        post_urls = set()

        # æ­£åˆ™åŒ¹é…ï¼šhttps://freefq.com/free-ssr/2025/06/03/ssr.html è¿™ç§æ ¼å¼
        pattern = re.compile(r'https?://freefq\.com/free-ssr/\d{4}/\d{2}/\d{2}/[^\'"<> ]+\.html')
        for m in pattern.finditer(html_source):
            post_urls.add(m.group(0).strip())

        logger.info(f"ğŸ“° åœ¨åˆ†ç±»é¡µå…±åŒ¹é…åˆ° {len(post_urls)} ç¯‡çœŸå®æ–‡ç« ")
        return list(post_urls)

    except Exception as e:
        logger.error(f"âŒ æå–åˆ†ç±»é¡µæ–‡ç« é“¾æ¥å¤±è´¥: {str(e)}")
        return []


# ---------------------- æå–å•ç¯‡æ–‡ç« ä¸­çš„â€œå€™é€‰é“¾æ¥â€ ----------------------
async def extract_raw_links_from_post(asession, post_url):
    """
    æ¸²æŸ“å•æ¡â€œæ–‡ç« é¡µâ€ï¼Œä»ï¼š
      1. å®Œæ•´ HTML æºç 
      2. <a> æ ‡ç­¾çš„ href
      3. <code> / <pre> æ ‡ç­¾é‡Œçš„çº¯æ–‡æœ¬
    ä¸­æå–æ‰€æœ‰â€œå¸¦æœ‰ clash/v2ray/.yaml/vmess:// ç­‰å…³é”®è¯â€çš„åŸå§‹å€™é€‰é“¾æ¥ï¼Œ
    ä½†æ­¤å¤„ä¸åšæœ€ç»ˆéªŒè¯ï¼Œåªæ˜¯å…ˆæ”¶é›†æ‰€æœ‰å¯èƒ½çš„ URL æˆ– Base64 åè®®ä¸²ã€‚
    """
    raw_links = set()
    try:
        r = await asession.get(post_url, headers={"User-Agent": USER_AGENT})
        # æ¸²æŸ“é¡µé¢ï¼Œç­‰å¾… JS æ‰§è¡Œ
        await r.html.arender(timeout=30, sleep=2)

        html_source = r.html.html or ""

        # 1. ä»å®Œæ•´ HTML æºç ç”¨æ­£åˆ™æå–æ‰€æœ‰ http(s)://â€¦ æ ¼å¼çš„ URL
        for match in re.findall(r'https?://[^\s\'"<>()]+', html_source):
            # åªè¦é“¾æ¥é‡Œå«â€œclashâ€ã€â€œv2rayâ€ã€â€œ.yamlâ€ã€â€œ.txtâ€ã€â€œsubscribeâ€ç­‰å…³é”®è¯ï¼Œå°±å…ˆæ”¶é›†
            if any(k in match for k in ['clash', 'v2ray', '.yaml', '.txt', 'subscribe']):
                raw_links.add(match.strip())

        # 2. ä»æºç ä¸­æå– Base64 èŠ‚ç‚¹åè®®ï¼ˆssr://ã€ss://ã€vmess://ã€trojan://ï¼‰
        for m in re.finditer(r'(ssr|ss|vmess|trojan)://[A-Za-z0-9+/=]+', html_source):
            raw_links.add(m.group(0).strip())

        # 3. å•ç‹¬éå† <a> æ ‡ç­¾çš„ href å±æ€§ï¼Œé¿å…æŸäº›é“¾æ¥åªå­˜åœ¨äºå±æ€§å€¼ä¸­è€Œä¸åœ¨æ–‡æœ¬é‡Œ
        for a in r.html.find('a'):
            href = a.attrs.get('href', '').strip()
            if href and any(k in href for k in ['clash', 'v2ray', '.yaml', '.txt', 'subscribe', 'ssr://', 'vmess://']):
                full = href if href.startswith('http') else urljoin(post_url, href)
                raw_links.add(full.strip())

        # 4. éå† <code> å’Œ <pre> æ ‡ç­¾å†…çš„æ–‡æœ¬ï¼Œæœ‰æ—¶èŠ‚ç‚¹é“¾æ¥ä»¥çº¯æ–‡æœ¬å½¢å¼æ’å…¥
        for block in r.html.find('code, pre'):
            text = block.text or ""
            # 4.1 æå– Base64 æ ¼å¼åè®®
            for m in re.finditer(r'(ssr|ss|vmess|trojan)://[A-Za-z0-9+/=]+', text):
                raw_links.add(m.group(0).strip())
            # 4.2 æå– http(s)://â€¦ æ ¼å¼
            for match in re.findall(r'https?://[^\s\'"<>()]+', text):
                if any(k in match for k in ['clash', 'v2ray', '.yaml', '.txt', 'subscribe']):
                    raw_links.add(match.strip())

        logger.info(f"   ğŸ”— æ–‡ç«  {post_url} å…±æ”¶é›†åˆ° {len(raw_links)} æ¡â€œå€™é€‰â€é“¾æ¥")
        return list(raw_links)

    except Exception as e:
        logger.error(f"âŒ æå–æ–‡ç«  {post_url} å€™é€‰é“¾æ¥å¤±è´¥: {str(e)}")
        return []


# ---------------------- éªŒè¯å¹¶ç­›é€‰å‡ºâ€œçœŸå®æœ‰æ•ˆè®¢é˜…é“¾æ¥â€ ----------------------
def filter_and_validate_links(raw_links):
    """
    1. å…ˆç”¨ is_subscription_link() ç­›é€‰â€œå½¢å¼ä¸Šåƒè®¢é˜…â€çš„URL
    2. å†ç”¨ requests.get éªŒè¯ HTTP çŠ¶æ€ç  = 200ï¼Œä¸”å†…å®¹ä¸­å«â€œproxies/vmess/ss:///trojan/vless/clashâ€ç­‰å…³é”®è¯ï¼Œ
       æˆ–è€…è¿”å›ä½“æœ¬èº«å°±æ˜¯çº¯ Base64ã€‚
    è¿”å›æœ€ç»ˆâ€œæœ‰æ•ˆå¯ç”¨â€çš„è®¢é˜…é“¾æ¥åˆ—è¡¨ã€‚
    """
    filtered = []
    for link in raw_links:
        if not is_subscription_link(link):
            continue

        # è§„èŒƒåŒ– URLï¼Œæ–¹ä¾¿åç»­è¯·æ±‚
        url = link
        if url.startswith('//'):
            url = 'https:' + url
        elif not url.startswith('http'):
            url = 'https://' + url

        try:
            time.sleep(1)  # éšæœºå»¶è¿Ÿ
            resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=10)
            if resp.status_code != 200:
                logger.warning(f"    âŒ HTTP {resp.status_code}: {url}")
                continue

            text = resp.text.lower()
            if any(k in text for k in ['proxies', 'vmess', 'ss://', 'trojan', 'vless', 'clash']):
                logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜…: {url}")
                filtered.append(url)
            elif re.fullmatch(r'[A-Za-z0-9+/=]+', text.strip()):
                logger.info(f"    âœ”ï¸ æœ‰æ•ˆBase64: {url}")
                filtered.append(url)
            else:
                logger.warning(f"    âŒ æ— æœ‰æ•ˆé…ç½®: {url}")

        except Exception as e:
            logger.warning(f"âš ï¸ éªŒè¯å¼‚å¸¸: {url} - {str(e)}")

    return filtered


# ---------------------- å¼‚æ­¥æ¨é€åˆ° Telegram ----------------------
async def send_to_telegram(bot_token: str, channel_id: str, urls: list):
    """
    å°†å‰ 10 æ¡æœ‰æ•ˆè®¢é˜… URL ä»¥ HTML æ ¼å¼å¼‚æ­¥æ¨é€è‡³ Telegram é¢‘é“/ç¾¤ç»„ã€‚
    """
    if not urls:
        logger.warning("âŒ æ— æœ‰æ•ˆé“¾æ¥ï¼Œè·³è¿‡æ¨é€")
        return

    text = "ğŸŒ <b>FreeFQ æœ€æ–°å…è´¹èŠ‚ç‚¹è®¢é˜…</b>\n\n"
    text += f"æ›´æ–°æ—¶é—´ï¼š<code>{time.strftime('%Y-%m-%d %H:%M:%S')}</code>\n\n"

    for i, link in enumerate(urls[:10], 1):
        safe = quote(link, safe=":/?=&")
        snippet = link if len(link) <= 60 else (link[:57] + "...")
        text += f"{i}. <code>{snippet}</code>\n"
        text += f"   ğŸ‘‰ <a href=\"{safe}\">ç‚¹æˆ‘ä½¿ç”¨</a>\n\n"

    text += "âš ï¸ ä»…ä¾›å­¦ä¹ ç”¨é€”ï¼Œè¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„"

    bot = Bot(token=bot_token)
    try:
        await bot.send_message(
            chat_id=channel_id,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        logger.info("âœ… Telegram æ¨é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Telegram æ¨é€å¤±è´¥: {str(e)}")


# ---------------------- ä¸»æµç¨‹ ----------------------
async def main():
    logger.info("ğŸš€ FreeFQ èŠ‚ç‚¹çˆ¬è™«å¯åŠ¨")
    asession = AsyncHTMLSession()
    try:
        # 1. ä»åˆ†ç±»é¡µæå–æ‰€æœ‰â€œçœŸå®æ–‡ç« â€é“¾æ¥
        post_urls = await extract_post_urls(asession)

        # 2. é’ˆå¯¹æœ€æ–° 5 ç¯‡æ–‡ç« ï¼ˆå¯æ ¹æ®éœ€æ±‚å¢å‡ï¼‰ï¼Œæ”¶é›†å®ƒä»¬æ‰€æœ‰çš„â€œå€™é€‰åŸå§‹é“¾æ¥â€
        all_raw_links = set()
        for post_url in sorted(post_urls, reverse=True)[:5]:
            raw = await extract_raw_links_from_post(asession, post_url)
            all_raw_links.update(raw)
        logger.info(f"\nğŸ” å…±æ”¶é›†åˆ° {len(all_raw_links)} æ¡â€œå€™é€‰â€é“¾æ¥ï¼Œå¼€å§‹ç­›é€‰ä¸éªŒè¯â€¦")

        # 3. ç­›é€‰å¹¶éªŒè¯â€œçœŸæ­£çš„æœ‰æ•ˆè®¢é˜…é“¾æ¥â€
        valid_links = filter_and_validate_links(list(all_raw_links))
        logger.info(f"\nâœ”ï¸ éªŒè¯å®Œæˆï¼å…± {len(valid_links)} æ¡æœ‰æ•ˆè®¢é˜…é“¾æ¥")

        # 4. ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if valid_links:
            with open("freefq_valid_links.txt", "w", encoding="utf-8") as f:
                for v in valid_links:
                    f.write(v + "\n")
            logger.info("ğŸ“„ å·²ä¿å­˜åˆ° freefq_valid_links.txt")

        # 5. å¦‚æœé…ç½®äº† BOT_TOKEN & CHANNEL_IDï¼Œåˆ™å¼‚æ­¥æ¨é€åˆ° Telegram
        if BOT_TOKEN and CHANNEL_ID:
            await send_to_telegram(BOT_TOKEN, CHANNEL_ID, valid_links)
        else:
            logger.warning("âŒ æœªé…ç½® BOT_TOKEN æˆ– CHANNEL_IDï¼Œè·³è¿‡ Telegram æ¨é€")

    finally:
        # 6. æ˜¾å¼å…³é—­ AsyncHTMLSessionï¼Œé¿å…â€œEvent loop is closedâ€æŠ¥é”™
        try:
            await asession.close()
            logger.info("â„¹ï¸ AsyncHTMLSession å·²å…³é—­")
        except Exception:
            pass


if __name__ == "__main__":
    asyncio.run(main())
