import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
from urllib.parse import urljoin, quote
import time
import random
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
BASE_URL = "https://nodefree.net"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0",
    "TE": "Trailers"
}

def get_threads_on_page(page):
    """
    ä» nodefree.net åˆ—è¡¨é¡µä¸­æå–æ‰€æœ‰æ–‡ç« é“¾æ¥
    """
    # ä¿®æ­£åˆ†é¡µURLæ ¼å¼
    if page == 1:
        url = BASE_URL
    else:
        url = f"{BASE_URL}/page/{page}"
    
    logger.info(f"ğŸ” æ­£åœ¨çˆ¬å–é¡µé¢: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(resp.text, 'html.parser')
        threads = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« é“¾æ¥ - æ ¹æ®å½“å‰ç½‘ç«™ç»“æ„è°ƒæ•´
        # å°è¯•å¤šç§é€‰æ‹©å™¨ä»¥ç¡®ä¿æ‰¾åˆ°æ–‡ç« é“¾æ¥
        selectors = [
            'a.list-group-item',  # åŸå§‹é€‰æ‹©å™¨
            'div.topic-list-item a.title',  # å¤‡é€‰é€‰æ‹©å™¨1
            'article a[href^="/p/"]',  # å¤‡é€‰é€‰æ‹©å™¨2
            'a[href*="/p/"]'  # é€šç”¨é€‰æ‹©å™¨
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href')
                if href and '/p/' in href:
                    full_url = urljoin(BASE_URL, href)
                    if full_url not in threads:
                        threads.append(full_url)
            if threads:
                break
        
        logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ç¯‡æ–‡ç« ")
        return threads
    
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            logger.warning(f"âš ï¸ é¡µé¢ä¸å­˜åœ¨: {url}")
        else:
            logger.error(f"âš ï¸ HTTPé”™è¯¯ {e.response.status_code}: {url}")
        return []
    except Exception as e:
        logger.error(f"âš ï¸ è·å–é¡µé¢å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def extract_yaml_links_from_thread(url):
    """
    ä»å•ä¸ªæ–‡ç« é¡µé¢ä¸­æå–æ‰€æœ‰è®¢é˜…é“¾æ¥
    """
    logger.info(f"ğŸ“ æ­£åœ¨è§£ææ–‡ç« : {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = set()
        
        # æå–æ‰€æœ‰å¯èƒ½çš„è®¢é˜…é“¾æ¥
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            # åŒ¹é….yaml/.ymlæ–‡ä»¶
            if re.search(r'\.ya?ml$', href, re.I):
                links.add(href)
            
            # åŒ¹é….txtæ–‡ä»¶
            elif re.search(r'\.txt$', href, re.I):
                links.add(href)
            
            # åŒ¹é…å¸¸è§è®¢é˜…æœåŠ¡åŸŸå
            elif any(domain in href for domain in 
                    ['githubrowcontent', 'github.io', 'sub-store', 'subscribe', 'clash', 'v2ray', 'youlink']):
                links.add(href)
        
        # æ£€æŸ¥æ–‡ç« å†…å®¹ä¸­çš„ç›´æ¥é“¾æ¥
        content_selectors = ['div.content', 'div.article-content', 'div.post-content']
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content:
                text_links = re.findall(r'https?://[^\s"\']+', content.text)
                for link in text_links:
                    if any(ext in link for ext in ['.yaml', '.yml', '.txt', 'sub-store', 'clash', 'v2ray']):
                        links.add(link)
        
        # æ£€æŸ¥ä»£ç å—ä¸­çš„é“¾æ¥
        for code_block in soup.select('pre, code'):
            code_links = re.findall(r'https?://[^\s"\']+', code_block.text)
            for link in code_links:
                if any(ext in link for ext in ['.yaml', '.yml', '.txt']):
                    links.add(link)
        
        logger.info(f"   ğŸ”— æå–åˆ° {len(links)} ä¸ªè®¢é˜…é“¾æ¥")
        return list(links)
    
    except Exception as e:
        logger.error(f"âš ï¸ è§£æå¸–å­å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def validate_subscription(url):
    """
    éªŒè¯è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ
    """
    logger.info(f"ğŸ” æ­£åœ¨éªŒè¯é“¾æ¥: {url}")
    
    try:
        # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°
        time.sleep(random.uniform(0.5, 1.5))
        
        # å¤„ç†å¯èƒ½çš„ç›¸å¯¹URL
        if url.startswith('//'):
            url = 'https:' + url
        elif url.startswith('/'):
            url = urljoin(BASE_URL, url)
            
        res = requests.get(url, headers=HEADERS, timeout=15)
        if res.status_code != 200:
            logger.warning(f"    âŒ HTTP {res.status_code}: {url}")
            return False
        
        content = res.text
        # æ£€æŸ¥å¸¸è§VPNé…ç½®å…³é”®è¯
        if any(keyword in content.lower() for keyword in 
               ["proxies", "proxy-providers", "vmess", "ss://", "trojan", "vless", "clash"]):
            logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜…: {url}")
            return True
        
        logger.warning(f"    âŒ æ— æ•ˆè®¢é˜… (æ— VPNé…ç½®): {url}")
        return False
    
    except Exception as e:
        logger.error(f"    âŒ éªŒè¯å¼‚å¸¸: {url}ï¼Œé”™è¯¯: {str(e)}")
        return False

async def send_to_telegram(bot_token, channel_id, urls):
    """
    å°†æœ‰æ•ˆè®¢é˜…é“¾æ¥æ¨é€åˆ° Telegram é¢‘é“
    """
    if not urls:
        logger.warning("âŒ æ— æœ‰æ•ˆé“¾æ¥ï¼Œè·³è¿‡æ¨é€")
        return
    
    # åˆ›å»ºæ¶ˆæ¯å†…å®¹
    text = "ğŸ†• <b>NodeFree æœ€æ–°å…è´¹VPNè®¢é˜…åˆé›†</b>\n\n"
    text += "æ›´æ–°æ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n\n"
    
    for i, u in enumerate(urls[:15], 1):
        safe = quote(u, safe=":/?=&")
        # ç¼©çŸ­æ˜¾ç¤ºçš„é•¿é“¾æ¥
        display_url = u
        if len(u) > 50:
            display_url = u[:30] + "..." + u[-20:]
        text += f"{i}. <a href=\"{safe}\">{display_url}</a>\n"
    
    text += "\nâš ï¸ ä»…ä¾›å­¦ä¹ ä½¿ç”¨ï¼Œè¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„"
    
    # æ§åˆ¶æ¶ˆæ¯é•¿åº¦
    if len(text) > 4096:
        text = text[:4000] + "\n\n...ï¼ˆéƒ¨åˆ†é“¾æ¥å·²çœç•¥ï¼‰"
    
    bot = Bot(token=bot_token)
    try:
        await bot.send_message(
            chat_id=channel_id,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True,
        )
        logger.info("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨é€å¤±è´¥: {str(e)}")

async def main():
    logger.info("="*50)
    logger.info(f"ğŸŒ NodeFree å…è´¹èŠ‚ç‚¹çˆ¬è™«å¯åŠ¨ - {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*50)
    
    all_links = set()
    valid_links = []
    
    # çˆ¬å–å‰3é¡µå†…å®¹
    for page in range(1, 4):
        threads = get_threads_on_page(page)
        
        if not threads:
            logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« ï¼Œè·³è¿‡")
            continue
            
        # éšæœºå»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(random.uniform(1, 3))
        
        for t in threads:
            subs = extract_yaml_links_from_thread(t)
            all_links.update(subs)
            
            # éšæœºå»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(0.5, 2))
    
    logger.info(f"\nğŸ” å…±æå–åˆ° {len(all_links)} æ¡è®¢é˜…é“¾æ¥ï¼Œå¼€å§‹éªŒè¯...")
    
    for link in all_links:
        if validate_subscription(link):
            valid_links.append(link)
    
    logger.info(f"\nâœ”ï¸ éªŒè¯å®Œæˆï¼å…± {len(valid_links)} æ¡æœ‰æ•ˆè®¢é˜…é“¾æ¥")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if valid_links:
        with open("valid_links.txt", "w", encoding="utf-8") as f:
            for v in valid_links:
                f.write(v + "\n")
        logger.info("ğŸ“„ ç»“æœå·²ä¿å­˜åˆ° valid_links.txt")
    else:
        logger.warning("ğŸ“„ æ— æœ‰æ•ˆé“¾æ¥ï¼Œä¸ä¿å­˜æ–‡ä»¶")
    
    # å‘é€åˆ°Telegram
    if BOT_TOKEN and CHANNEL_ID:
        logger.info("\nğŸ“¤ æ­£åœ¨æ¨é€ç»“æœåˆ°Telegram...")
        await send_to_telegram(BOT_TOKEN, CHANNEL_ID, valid_links)
    
    logger.info("\nâœ… ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
