import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
from urllib.parse import urljoin, quote
import time
import random
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
BASE_URL = "https://nodefree.net"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0",
    "TE": "Trailers"
}

def get_threads_on_page(page):
    """
    ä» nodefree.net åˆ—è¡¨é¡µä¸­æå–æ‰€æœ‰æ–‡ç« é“¾æ¥ - æ”¹è¿›ç‰ˆ
    """
    if page == 1:
        url = BASE_URL
    else:
        url = f"{BASE_URL}/page/{page}"
    
    logger.info(f"ğŸ” æ­£åœ¨çˆ¬å–é¡µé¢: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        threads = []
        
        # æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨ï¼šæŸ¥æ‰¾æ–‡ç« æ ‡é¢˜é“¾æ¥
        for article in soup.select('div.topic-list-item'):
            title_link = article.select_one('a.title')
            if title_link:
                href = title_link.get('href')
                if href:
                    full_url = urljoin(BASE_URL, href)
                    threads.append(full_url)
        
        logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ç¯‡æ–‡ç« ")
        return threads
    
    except Exception as e:
        logger.error(f"âš ï¸ è·å–é¡µé¢å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def extract_subscription_links(url):
    """
    ä»å•ä¸ªæ–‡ç« é¡µé¢ä¸­æå–è®¢é˜…é“¾æ¥ - ç›´æ¥æ–¹æ³•
    """
    logger.info(f"ğŸ“ æ­£åœ¨è§£ææ–‡ç« : {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = set()
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰ç›´æ¥é“¾æ¥
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            # è¯†åˆ«å¸¸è§è®¢é˜…é“¾æ¥æ¨¡å¼
            if any(pattern in href for pattern in 
                  ['.yaml', '.yml', '.txt', 'clash', 'v2ray', 'subscribe', 'nodefree']):
                # å¤„ç†ç›¸å¯¹URL
                if not href.startswith('http'):
                    href = urljoin(url, href)
                links.add(href)
        
        # 2. åœ¨æ–‡ç« å†…å®¹ä¸­æœç´¢é“¾æ¥
        content = soup.select_one('div.content') or soup.select_one('div.post-content')
        if content:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„URL
            potential_links = re.findall(r'https?://[^\s"\']+', content.text)
            for link in potential_links:
                # è¿‡æ»¤å‡ºå¯èƒ½çš„è®¢é˜…é“¾æ¥
                if any(pattern in link for pattern in 
                      ['.yaml', '.yml', '.txt', 'clash', 'v2ray', 'subscribe', 'nodefree']):
                    links.add(link)
        
        # 3. åœ¨ä»£ç å—ä¸­æœç´¢é…ç½®
        for code_block in soup.select('pre, code'):
            # æŸ¥æ‰¾base64ç¼–ç çš„é…ç½®
            base64_links = re.findall(
                r'(?:ss|ssr|vmess|trojan)://[a-zA-Z0-9+/]+={0,2}', 
                code_block.text
            )
            links.update(base64_links)
            
            # æŸ¥æ‰¾æ–‡æœ¬æ ¼å¼çš„é…ç½®
            text_links = re.findall(r'https?://[^\s"\']+', code_block.text)
            for link in text_links:
                if any(pattern in link for pattern in 
                      ['.yaml', '.yml', '.txt', 'clash', 'v2ray', 'subscribe']):
                    links.add(link)
        
        logger.info(f"   ğŸ”— æå–åˆ° {len(links)} ä¸ªè®¢é˜…é“¾æ¥")
        return list(links)
    
    except Exception as e:
        logger.error(f"âš ï¸ è§£æå¸–å­å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def validate_subscription(url):
    """
    éªŒè¯è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ - æ”¹è¿›ç‰ˆ
    """
    logger.info(f"ğŸ” æ­£åœ¨éªŒè¯é“¾æ¥: {url}")
    
    try:
        # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°
        time.sleep(random.uniform(0.5, 1.5))
        
        # å¤„ç†å¯èƒ½çš„ç›¸å¯¹URL
        if url.startswith('//'):
            url = 'https:' + url
        elif not url.startswith('http'):
            url = 'https://' + url
            
        res = requests.get(url, headers=HEADERS, timeout=15)
        if res.status_code != 200:
            logger.warning(f"    âŒ HTTP {res.status_code}: {url}")
            return False
        
        content = res.text
        
        # æ£€æŸ¥å¸¸è§VPNé…ç½®å…³é”®è¯
        vpn_keywords = [
            "proxies", "proxy-providers", "vmess", "ss://", 
            "trojan", "vless", "clash", "port:"
        ]
        
        for keyword in vpn_keywords:
            if keyword.lower() in content.lower():
                logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜…: {url}")
                return True
        
        # æ£€æŸ¥base64ç¼–ç çš„é…ç½®
        if re.search(r'^[A-Za-z0-9+/]+={0,2}$', content.strip()):
            logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜… (Base64ç¼–ç ): {url}")
            return True
        
        logger.warning(f"    âŒ æ— æ•ˆè®¢é˜… (æ— VPNé…ç½®): {url}")
        return False
    
    except Exception as e:
        logger.error(f"    âŒ éªŒè¯å¼‚å¸¸: {url}ï¼Œé”™è¯¯: {str(e)}")
        return False

async def send_to_telegram(bot_token, channel_id, urls):
    """
    å°†æœ‰æ•ˆè®¢é˜…é“¾æ¥æ¨é€åˆ° Telegram é¢‘é“
    """
    if not urls:
        logger.warning("âŒ æ— æœ‰æ•ˆé“¾æ¥ï¼Œè·³è¿‡æ¨é€")
        return
    
    # åˆ›å»ºæ¶ˆæ¯å†…å®¹
    text = "ğŸ†• <b>NodeFree æœ€æ–°å…è´¹VPNè®¢é˜…é“¾æ¥</b>\n\n"
    text += "æ›´æ–°æ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n\n"
    text += "ä»¥ä¸‹æ˜¯å¯ç›´æ¥å¯¼å…¥VPNå®¢æˆ·ç«¯çš„è®¢é˜…é“¾æ¥ï¼š\n\n"
    
    for i, u in enumerate(urls[:10], 1):
        safe = quote(u, safe=":/?=&")
        # æ˜¾ç¤ºé“¾æ¥çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ ‡è¯†
        display_name = u.split('/')[-1] if '/' in u else u
        text += f"{i}. <code>{display_name}</code>\n"
        text += f"   <a href=\"{safe}\">ç‚¹å‡»å¤åˆ¶è®¢é˜…é“¾æ¥</a>\n\n"
    
    text += "âš ï¸ ä»…ä¾›å­¦ä¹ ä½¿ç”¨ï¼Œè¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„\n"
    text += "ğŸ”’ è®¢é˜…é“¾æ¥æœ‰æ•ˆæœŸé€šå¸¸ä¸º1-7å¤©"
    
    # æ§åˆ¶æ¶ˆæ¯é•¿åº¦
    if len(text) > 4096:
        text = text[:4000] + "\n\n...ï¼ˆéƒ¨åˆ†é“¾æ¥å·²çœç•¥ï¼‰"
    
    bot = Bot(token=bot_token)
    try:
        await bot.send_message(
            chat_id=channel_id,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True,
        )
        logger.info("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨é€å¤±è´¥: {str(e)}")

async def main():
    logger.info("="*50)
    logger.info(f"ğŸŒ NodeFree å…è´¹èŠ‚ç‚¹çˆ¬è™«å¯åŠ¨ - {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*50)
    
    all_links = set()
    valid_links = []
    
    # çˆ¬å–å‰2é¡µå†…å®¹
    for page in range(1, 3):
        threads = get_threads_on_page(page)
        
        if not threads:
            logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« ï¼Œè·³è¿‡")
            continue
            
        time.sleep(random.uniform(1, 3))
        
        for t in threads:
            subs = extract_subscription_links(t)
            all_links.update(subs)
            
            time.sleep(random.uniform(0.5, 2))
    
    logger.info(f"\nğŸ” å…±æå–åˆ° {len(all_links)} æ¡è®¢é˜…é“¾æ¥ï¼Œå¼€å§‹éªŒè¯...")
    
    # éªŒè¯æ‰€æœ‰é“¾æ¥
    for link in all_links:
        if validate_subscription(link):
            valid_links.append(link)
    
    logger.info(f"\nâœ”ï¸ éªŒè¯å®Œæˆï¼å…± {len(valid_links)} æ¡æœ‰æ•ˆè®¢é˜…é“¾æ¥")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if valid_links:
        with open("valid_links.txt", "w", encoding="utf-8") as f:
            for v in valid_links:
                f.write(v + "\n")
        logger.info("ğŸ“„ ç»“æœå·²ä¿å­˜åˆ° valid_links.txt")
    else:
        logger.warning("ğŸ“„ æ— æœ‰æ•ˆé“¾æ¥ï¼Œä¸ä¿å­˜æ–‡ä»¶")
    
    # å‘é€åˆ°Telegram
    if BOT_TOKEN and CHANNEL_ID and valid_links:
        logger.info("\nğŸ“¤ æ­£åœ¨æ¨é€ç»“æœåˆ°Telegram...")
        await send_to_telegram(BOT_TOKEN, CHANNEL_ID, valid_links)
    elif valid_links:
        logger.warning("\nâŒ æœªè®¾ç½®BOT_TOKENæˆ–CHANNEL_IDï¼Œè·³è¿‡æ¨é€")
    
    logger.info("\nâœ… ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
