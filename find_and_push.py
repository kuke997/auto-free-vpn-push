import os
import re
import requests
import asyncio
import yaml
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
import urllib.parse

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

BASE_URL = "https://nodefree.net"


def extract_sub_links_from_page(url):
    """
    ä»ä¸€ä¸ªç½‘é¡µä¸­æå–æ‰€æœ‰å¸¦ .yaml .yml æˆ–åŒ…å« clash çš„é“¾æ¥ï¼Œè¿”å›åˆ—è¡¨
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        found_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if re.search(r"\.ya?ml", href, re.I) or re.search(r"clash", href, re.I):
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = BASE_URL + href
                found_links.add(href)
        return list(found_links)
    except Exception as e:
        print(f"âš ï¸ ä»é¡µé¢ {url} æå–è®¢é˜…é“¾æ¥å¤±è´¥: {e}")
        return []


def fetch_nodefree_links():
    """
    æŠ“å– nodefree.net é¦–é¡µï¼Œå…ˆæå–ä¸»é¡µçš„æ‰€æœ‰å¯èƒ½é“¾æ¥ï¼Œ
    å¦‚æœæ˜¯é…ç½®æ–‡ä»¶é“¾æ¥ï¼ˆç›´æ¥.yamlï¼‰ï¼Œç›´æ¥åŠ å…¥ç»“æœï¼Œ
    å¦‚æœæ˜¯ç½‘é¡µï¼Œè¿›ä¸€æ­¥è®¿é—®è§£æé‡Œé¢çš„é…ç½®æ–‡ä»¶é“¾æ¥
    """
    print("ğŸŒ æ­£åœ¨æŠ“å– nodefree.net é¦–é¡µæ‰€æœ‰å¯èƒ½çš„è®¢é˜…é“¾æ¥...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(BASE_URL, headers=headers, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        candidate_links = set()

        # å…ˆä»é¦–é¡µæå–æ‰€æœ‰aé“¾æ¥ï¼Œæ‰¾å«.yaml/.ymlæˆ–clashçš„é“¾æ¥ï¼ˆç½‘é¡µå’Œæ–‡ä»¶å‡å¯èƒ½ï¼‰
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if re.search(r"\.ya?ml", href, re.I) or re.search(r"clash", href, re.I):
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = BASE_URL + href
                candidate_links.add(href)

        print(f"ğŸ· é¦–é¡µå…±å‘ç° {len(candidate_links)} ä¸ªå¯èƒ½çš„è®¢é˜…é“¾æ¥æˆ–ç½‘é¡µ")

        # è¿›ä¸€æ­¥åˆ†ç±»
        final_links = set()

        for link in candidate_links:
            # åˆ¤æ–­æ˜¯ä¸æ˜¯ç›´æ¥.yamlæ–‡ä»¶é“¾æ¥
            if re.search(r"\.ya?ml$", link, re.I):
                final_links.add(link)
            else:
                # ä¸æ˜¯ç›´æ¥é…ç½®æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ç½‘é¡µï¼Œè®¿é—®å®ƒï¼Œè§£æé‡Œé¢çš„è®¢é˜…é“¾æ¥
                print(f"ğŸ” è®¿é—®ç½‘é¡µ {link}ï¼Œå°è¯•æå–å†…éƒ¨è®¢é˜…é“¾æ¥")
                inner_links = extract_sub_links_from_page(link)
                for l in inner_links:
                    final_links.add(l)

        print(f"âœ… æ€»å…±æœ€ç»ˆè®¢é˜…é“¾æ¥æ•°é‡ï¼š{len(final_links)}")
        return list(final_links)
    except Exception as e:
        print("âŒ æŠ“å–å¤±è´¥:", e)
        return []


def validate_subscription(url):
    try:
        res = requests.get(url, timeout=10)
        if res.status_code != 200:
            return False
        text = res.text.lower()
        if "proxies" in text or "vmess://" in text or "ss://" in text or "clash" in text:
            return True
        return False
    except Exception:
        return False


def get_subscription_country_info(url):
    try:
        res = requests.get(url, timeout=10)
        if res.status_code != 200:
            return None
        data = yaml.safe_load(res.text)
        proxies = data.get("proxies", [])
        countries = set()
        for proxy in proxies:
            country = proxy.get("country") or proxy.get("region")
            if country and isinstance(country, str) and len(country) <= 5:
                countries.add(country.strip())
                continue
            name = proxy.get("name") or proxy.get("remark") or proxy.get("remarks")
            if name and isinstance(name, str) and len(name) >= 2:
                countries.add(name[:2].strip())
        return ", ".join(sorted(countries)) if countries else None
    except Exception as e:
        print(f"âš ï¸ åœ°åŒºè§£æå¤±è´¥: {url}, é”™è¯¯: {e}")
        return None


async def send_to_telegram(bot_token, channel_id, urls):
    if not urls:
        print("âŒ æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œè·³è¿‡æ¨é€")
        return

    text = "ğŸ†• <b>2025å¹´æœ€æ–°å…è´¹VPNèŠ‚ç‚¹åˆé›†ï¼ˆClash/V2Ray/SSï¼‰</b>\n\n"
    for i, url in enumerate(urls[:20], start=1):
        country_info = get_subscription_country_info(url)
        if country_info:
            country_info = f"ï¼ˆåœ°åŒº: {country_info}ï¼‰"
        else:
            country_info = ""
        safe_url = urllib.parse.quote(url, safe=":/?=&")
        text += f"ğŸ‘‰ <a href=\"{safe_url}\">{url}</a> {country_info}\n\n"

    if len(text.encode("utf-8")) > 4000:
        text = text.encode("utf-8")[:4000].decode("utf-8", errors="ignore") + "\n..."

    bot = Bot(token=bot_token)
    try:
        await bot.send_message(chat_id=channel_id, text=text, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print("âŒ æ¨é€å¤±è´¥:", e)


async def main():
    if not BOT_TOKEN or not CHANNEL_ID:
        print("âŒ æœªè®¾ç½® BOT_TOKEN æˆ– CHANNEL_ID")
        return

    links = fetch_nodefree_links()
    print("ğŸ” éªŒè¯è®¢é˜…é“¾æ¥...")
    valid_links = [url for url in links if validate_subscription(url)]
    print(f"âœ”ï¸ æœ‰æ•ˆè®¢é˜…é“¾æ¥æ•°é‡: {len(valid_links)}")

    with open("valid_links.txt", "w") as f:
        for link in valid_links:
            f.write(link + "\n")
    print("ğŸ“„ å·²ä¿å­˜åˆ° valid_links.txt")

    await send_to_telegram(BOT_TOKEN, CHANNEL_ID, valid_links)


if __name__ == "__main__":
    asyncio.run(main())
