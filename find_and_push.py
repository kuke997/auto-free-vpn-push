import os
import re
import asyncio
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
from urllib.parse import urljoin, quote
import time
import random
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
BASE_URL = "https://nodefree.net"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0",
    "TE": "Trailers"
}

def get_threads_on_page(page):
    """
    ä» nodefree.net åˆ—è¡¨é¡µä¸­æå–æ‰€æœ‰æ–‡ç« é“¾æ¥
    """
    if page == 1:
        url = BASE_URL
    else:
        url = f"{BASE_URL}/page/{page}"
    
    logger.info(f"ğŸ” æ­£åœ¨çˆ¬å–é¡µé¢: {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        threads = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« é“¾æ¥ - æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
        article_links = soup.select('a.list-group-item')
        if not article_links:
            # å¤‡ç”¨é€‰æ‹©å™¨
            article_links = soup.select('a[href*="/p/"]')
        
        for link in article_links:
            href = link.get('href')
            if href and '/p/' in href:
                full_url = urljoin(BASE_URL, href)
                if full_url not in threads:
                    threads.append(full_url)
        
        logger.info(f"âœ… æ‰¾åˆ° {len(threads)} ç¯‡æ–‡ç« ")
        return threads
    
    except Exception as e:
        logger.error(f"âš ï¸ è·å–é¡µé¢å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def extract_yaml_links_from_thread(url):
    """
    ä»å•ä¸ªæ–‡ç« é¡µé¢ä¸­æå–æ‰€æœ‰è®¢é˜…é“¾æ¥ - ä¿®å¤ç‰ˆ
    """
    logger.info(f"ğŸ“ æ­£åœ¨è§£ææ–‡ç« : {url}")
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        links = set()
        
        # åªæå–ç‰¹å®šç±»å‹çš„é“¾æ¥ï¼Œé¿å…æ–‡ç« é“¾æ¥
        domains = [
            'githubrowcontent', 'github.io', 'sub-store', 
            'subscribe', 'clash', 'v2ray', 'youlink',
            'raw.githubusercontent.com', 'cdn.jsdelivr.net'
        ]
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥å…ƒç´ 
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            # è·³è¿‡è®¨è®ºä¸»é¢˜é“¾æ¥
            if '/t/' in href or '/p/' in href:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¢é˜…é“¾æ¥
            if any(ext in href for ext in ['.yaml', '.yml', '.txt']):
                links.add(href)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šåŸŸå
            elif any(domain in href for domain in domains):
                links.add(href)
        
        # 2. æŸ¥æ‰¾å†…å®¹ä¸­çš„ç›´æ¥é“¾æ¥
        content = soup.select_one('div.content') or soup.select_one('div.post-content')
        if content:
            # æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è®¢é˜…é“¾æ¥
            text_links = re.findall(r'https?://[^\s"\']+?\.(?:ya?ml|txt)\b', content.text, re.I)
            links.update(text_links)
            
            # åŒ¹é…ç‰¹å®šåŸŸåçš„é“¾æ¥
            domain_pattern = r'https?://(?:{})[^\s"\']+'.format('|'.join(domains))
            domain_links = re.findall(domain_pattern, content.text, re.I)
            links.update(domain_links)
        
        # 3. æŸ¥æ‰¾ä»£ç å—ä¸­çš„è®¢é˜…é“¾æ¥
        for code_block in soup.select('pre, code'):
            code_text = code_block.get_text()
            # åŒ¹é…å¸¸è§çš„è®¢é˜…æ ¼å¼
            config_links = re.findall(
                r'https?://[^\s"\']+?\.(?:ya?ml|txt)\b', 
                code_text, 
                re.I
            )
            links.update(config_links)
            
            # åŒ¹é…base64ç¼–ç çš„è®¢é˜…é“¾æ¥
            base64_links = re.findall(
                r'(?:ss|ssr|vmess|trojan)://[a-zA-Z0-9+/]+={0,2}', 
                code_text
            )
            links.update(base64_links)
        
        # è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥
        filtered_links = set()
        for link in links:
            # è·³è¿‡è®¨è®ºä¸»é¢˜é“¾æ¥
            if '/t/' in link or '/p/' in link:
                continue
                
            # ç¡®ä¿æ˜¯å®Œæ•´URL
            if link.startswith('//'):
                link = 'https:' + link
            elif link.startswith('/'):
                link = urljoin(BASE_URL, link)
                
            # ç¡®ä¿æ˜¯HTTP/HTTPSåè®®
            if link.startswith('http'):
                filtered_links.add(link)
        
        logger.info(f"   ğŸ”— æå–åˆ° {len(filtered_links)} ä¸ªè®¢é˜…é“¾æ¥")
        return list(filtered_links)
    
    except Exception as e:
        logger.error(f"âš ï¸ è§£æå¸–å­å¤±è´¥ {url} é”™è¯¯: {str(e)}")
        return []

def validate_subscription(url):
    """
    éªŒè¯è®¢é˜…é“¾æ¥æ˜¯å¦æœ‰æ•ˆ - æ›´ä¸¥æ ¼çš„éªŒè¯
    """
    logger.info(f"ğŸ” æ­£åœ¨éªŒè¯é“¾æ¥: {url}")
    
    try:
        time.sleep(random.uniform(0.5, 1.5))
        
        # å¤„ç†å¯èƒ½çš„ç›¸å¯¹URL
        if url.startswith('//'):
            url = 'https:' + url
        elif not url.startswith('http'):
            url = 'https://' + url
            
        res = requests.get(url, headers=HEADERS, timeout=15)
        if res.status_code != 200:
            logger.warning(f"    âŒ HTTP {res.status_code}: {url}")
            return False
        
        content = res.text
        
        # æ›´ä¸¥æ ¼çš„VPNé…ç½®æ£€æµ‹
        vpn_keywords = [
            "proxies:", "proxy-providers:", "vmess://", "ss://", 
            "trojan://", "vless://", "clash:", "port:"
        ]
        
        for keyword in vpn_keywords:
            if keyword in content.lower():
                logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜…: {url}")
                return True
        
        # æ£€æŸ¥base64ç¼–ç çš„é…ç½®
        if re.search(r'^[A-Za-z0-9+/]+={0,2}$', content.strip()):
            logger.info(f"    âœ”ï¸ æœ‰æ•ˆè®¢é˜… (Base64ç¼–ç ): {url}")
            return True
        
        logger.warning(f"    âŒ æ— æ•ˆè®¢é˜… (æ— VPNé…ç½®): {url}")
        return False
    
    except Exception as e:
        logger.error(f"    âŒ éªŒè¯å¼‚å¸¸: {url}ï¼Œé”™è¯¯: {str(e)}")
        return False

async def send_to_telegram(bot_token, channel_id, urls):
    """
    å°†æœ‰æ•ˆè®¢é˜…é“¾æ¥æ¨é€åˆ° Telegram é¢‘é“ - æ”¹è¿›ç‰ˆ
    """
    if not urls:
        logger.warning("âŒ æ— æœ‰æ•ˆé“¾æ¥ï¼Œè·³è¿‡æ¨é€")
        return
    
    # åˆ›å»ºæ¶ˆæ¯å†…å®¹
    text = "ğŸ†• <b>NodeFree æœ€æ–°å…è´¹VPNè®¢é˜…é“¾æ¥</b>\n\n"
    text += "æ›´æ–°æ—¶é—´: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n\n"
    text += "ä»¥ä¸‹æ˜¯å¯ç›´æ¥å¯¼å…¥VPNå®¢æˆ·ç«¯çš„è®¢é˜…é“¾æ¥ï¼š\n\n"
    
    for i, u in enumerate(urls[:10], 1):
        safe = quote(u, safe=":/?=&")
        # ç¼©çŸ­æ˜¾ç¤ºçš„é•¿é“¾æ¥
        display_url = u.split('/')[-1] if '/' in u else u  # æ˜¾ç¤ºæ–‡ä»¶åéƒ¨åˆ†
        text += f"{i}. <code>{display_url}</code>\n"
        text += f"   <a href=\"{safe}\">ç‚¹å‡»å¤åˆ¶è®¢é˜…é“¾æ¥</a>\n\n"
    
    text += "âš ï¸ ä»…ä¾›å­¦ä¹ ä½¿ç”¨ï¼Œè¯·éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„\n"
    text += "ğŸ”’ è®¢é˜…é“¾æ¥æœ‰æ•ˆæœŸé€šå¸¸ä¸º1-7å¤©"
    
    # æ§åˆ¶æ¶ˆæ¯é•¿åº¦
    if len(text) > 4096:
        text = text[:4000] + "\n\n...ï¼ˆéƒ¨åˆ†é“¾æ¥å·²çœç•¥ï¼‰"
    
    bot = Bot(token=bot_token)
    try:
        await bot.send_message(
            chat_id=channel_id,
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True,
        )
        logger.info("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨é€å¤±è´¥: {str(e)}")

async def main():
    logger.info("="*50)
    logger.info(f"ğŸŒ NodeFree å…è´¹èŠ‚ç‚¹çˆ¬è™«å¯åŠ¨ - {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*50)
    
    all_links = set()
    valid_links = []
    
    # çˆ¬å–å‰2é¡µå†…å®¹å³å¯
    for page in range(1, 3):
        threads = get_threads_on_page(page)
        
        if not threads:
            logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« ï¼Œè·³è¿‡")
            continue
            
        time.sleep(random.uniform(1, 3))
        
        for t in threads:
            subs = extract_yaml_links_from_thread(t)
            all_links.update(subs)
            
            time.sleep(random.uniform(0.5, 2))
    
    logger.info(f"\nğŸ” å…±æå–åˆ° {len(all_links)} æ¡è®¢é˜…é“¾æ¥ï¼Œå¼€å§‹éªŒè¯...")
    
    # ä¼˜å…ˆéªŒè¯ç‰¹å®šç±»å‹çš„é“¾æ¥
    for link in all_links:
        # ä¼˜å…ˆéªŒè¯.yaml/.ymlé“¾æ¥
        if any(ext in link for ext in ['.yaml', '.yml', '.txt']):
            if validate_subscription(link):
                valid_links.append(link)
    
    # ç„¶åéªŒè¯å…¶ä»–é“¾æ¥
    for link in all_links:
        if link not in valid_links:
            if validate_subscription(link):
                valid_links.append(link)
    
    logger.info(f"\nâœ”ï¸ éªŒè¯å®Œæˆï¼å…± {len(valid_links)} æ¡æœ‰æ•ˆè®¢é˜…é“¾æ¥")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if valid_links:
        with open("valid_links.txt", "w", encoding="utf-8") as f:
            for v in valid_links:
                f.write(v + "\n")
        logger.info("ğŸ“„ ç»“æœå·²ä¿å­˜åˆ° valid_links.txt")
    else:
        logger.warning("ğŸ“„ æ— æœ‰æ•ˆé“¾æ¥ï¼Œä¸ä¿å­˜æ–‡ä»¶")
    
    # å‘é€åˆ°Telegram
    if BOT_TOKEN and CHANNEL_ID and valid_links:
        logger.info("\nğŸ“¤ æ­£åœ¨æ¨é€ç»“æœåˆ°Telegram...")
        await send_to_telegram(BOT_TOKEN, CHANNEL_ID, valid_links)
    elif valid_links:
        logger.warning("\nâŒ æœªè®¾ç½®BOT_TOKENæˆ–CHANNEL_IDï¼Œè·³è¿‡æ¨é€")
    
    logger.info("\nâœ… ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
